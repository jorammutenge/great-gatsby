{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c89467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, auc\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7f2b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train set:  2206\n",
      " Original test set:  946\n",
      "After removing instances with no labels, train set size:  2206\n",
      " After removing instances with no labels, test set size:  946\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>reviewLength</th>\n",
       "      <th>OriginalReview</th>\n",
       "      <th>CleanedReview</th>\n",
       "      <th>TokenizedReview</th>\n",
       "      <th>StopwordRemovedReview</th>\n",
       "      <th>StemmedReview</th>\n",
       "      <th>ProcessedReview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>Perfect Reading Of The Perfect Book. I have al...</td>\n",
       "      <td>610</td>\n",
       "      <td>perfect reading of the perfect book. i have al...</td>\n",
       "      <td>perfect reading of the perfect book i have alw...</td>\n",
       "      <td>['perfect', 'reading', 'of', 'the', 'perfect',...</td>\n",
       "      <td>['perfect', 'reading', 'perfect', 'book', 'alw...</td>\n",
       "      <td>['perfect', 'read', 'of', 'the', 'perfect', 'b...</td>\n",
       "      <td>perfect read of the perfect book i have alway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Three Stars. Had writing into it, no pages mis...</td>\n",
       "      <td>52</td>\n",
       "      <td>three stars. had writing into it, no pages mis...</td>\n",
       "      <td>three stars had writing into it no pages missing</td>\n",
       "      <td>['three', 'stars', 'had', 'writing', 'into', '...</td>\n",
       "      <td>['three', 'stars', 'writing', 'pages', 'missing']</td>\n",
       "      <td>['three', 'star', 'had', 'write', 'into', 'it'...</td>\n",
       "      <td>three star had write into it no page miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>Possibly The Best Novel Ever Written.... Sigh....</td>\n",
       "      <td>627</td>\n",
       "      <td>possibly the best novel ever written.... sigh....</td>\n",
       "      <td>possibly the best novel ever written sigh gasp...</td>\n",
       "      <td>['possibly', 'the', 'best', 'novel', 'ever', '...</td>\n",
       "      <td>['possibly', 'best', 'novel', 'ever', 'written...</td>\n",
       "      <td>['possibl', 'the', 'best', 'novel', 'ever', 'w...</td>\n",
       "      <td>possibl the best novel ever written sigh gasp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Not my Favorite. I could never get into this b...</td>\n",
       "      <td>136</td>\n",
       "      <td>not my favorite. i could never get into this b...</td>\n",
       "      <td>not my favorite i could never get into this bo...</td>\n",
       "      <td>['not', 'my', 'favorite', 'i', 'could', 'never...</td>\n",
       "      <td>['favorite', 'could', 'never', 'get', 'book', ...</td>\n",
       "      <td>['not', 'my', 'favorit', 'i', 'could', 'never'...</td>\n",
       "      <td>not my favorit i could never get into thi boo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>Like 'Rating' one of the Gospels. I think I fi...</td>\n",
       "      <td>651</td>\n",
       "      <td>like 'rating' one of the gospels. i think i fi...</td>\n",
       "      <td>like rating one of the gospels i think i first...</td>\n",
       "      <td>['like', 'rating', 'one', 'of', 'the', 'gospel...</td>\n",
       "      <td>['like', 'rating', 'one', 'gospels', 'think', ...</td>\n",
       "      <td>['like', 'rate', 'one', 'of', 'the', 'gospel',...</td>\n",
       "      <td>like rate one of the gospel i think i first r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             review  reviewLength  \\\n",
       "0  positive  Perfect Reading Of The Perfect Book. I have al...           610   \n",
       "1   neutral  Three Stars. Had writing into it, no pages mis...            52   \n",
       "2  positive  Possibly The Best Novel Ever Written.... Sigh....           627   \n",
       "3   neutral  Not my Favorite. I could never get into this b...           136   \n",
       "4  positive  Like 'Rating' one of the Gospels. I think I fi...           651   \n",
       "\n",
       "                                      OriginalReview  \\\n",
       "0  perfect reading of the perfect book. i have al...   \n",
       "1  three stars. had writing into it, no pages mis...   \n",
       "2  possibly the best novel ever written.... sigh....   \n",
       "3  not my favorite. i could never get into this b...   \n",
       "4  like 'rating' one of the gospels. i think i fi...   \n",
       "\n",
       "                                       CleanedReview  \\\n",
       "0  perfect reading of the perfect book i have alw...   \n",
       "1  three stars had writing into it no pages missing    \n",
       "2  possibly the best novel ever written sigh gasp...   \n",
       "3  not my favorite i could never get into this bo...   \n",
       "4  like rating one of the gospels i think i first...   \n",
       "\n",
       "                                     TokenizedReview  \\\n",
       "0  ['perfect', 'reading', 'of', 'the', 'perfect',...   \n",
       "1  ['three', 'stars', 'had', 'writing', 'into', '...   \n",
       "2  ['possibly', 'the', 'best', 'novel', 'ever', '...   \n",
       "3  ['not', 'my', 'favorite', 'i', 'could', 'never...   \n",
       "4  ['like', 'rating', 'one', 'of', 'the', 'gospel...   \n",
       "\n",
       "                               StopwordRemovedReview  \\\n",
       "0  ['perfect', 'reading', 'perfect', 'book', 'alw...   \n",
       "1  ['three', 'stars', 'writing', 'pages', 'missing']   \n",
       "2  ['possibly', 'best', 'novel', 'ever', 'written...   \n",
       "3  ['favorite', 'could', 'never', 'get', 'book', ...   \n",
       "4  ['like', 'rating', 'one', 'gospels', 'think', ...   \n",
       "\n",
       "                                       StemmedReview  \\\n",
       "0  ['perfect', 'read', 'of', 'the', 'perfect', 'b...   \n",
       "1  ['three', 'star', 'had', 'write', 'into', 'it'...   \n",
       "2  ['possibl', 'the', 'best', 'novel', 'ever', 'w...   \n",
       "3  ['not', 'my', 'favorit', 'i', 'could', 'never'...   \n",
       "4  ['like', 'rate', 'one', 'of', 'the', 'gospel',...   \n",
       "\n",
       "                                     ProcessedReview  \n",
       "0   perfect read of the perfect book i have alway...  \n",
       "1          three star had write into it no page miss  \n",
       "2   possibl the best novel ever written sigh gasp...  \n",
       "3   not my favorit i could never get into thi boo...  \n",
       "4   like rate one of the gospel i think i first r...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing empty Review, train set size:  2206\n",
      " After removing empty Review, test set size:  945\n"
     ]
    }
   ],
   "source": [
    "#### LOAD DATASETS ####\n",
    "\n",
    "train_data_file = \"gatsby_train.csv\"\n",
    "test_data_file = \"gatsby_test.csv\"\n",
    "\n",
    "#Import train and test dataset into data frames and print out the original lengths\n",
    "train_data_df = pd.read_csv(train_data_file)\n",
    "test_data_df = pd.read_csv(test_data_file)\n",
    "print (\"Original train set: \",len(train_data_df))\n",
    "print (\" Original test set: \",len(test_data_df))\n",
    "\n",
    "# Remove rows with null labels\n",
    "train_data_df = train_data_df[~train_data_df[\"sentiment\"].isnull()]\n",
    "test_data_df = test_data_df[~test_data_df[\"sentiment\"].isnull()]\n",
    "print (\"After removing instances with no labels, train set size: \", len(train_data_df))\n",
    "print (\" After removing instances with no labels, test set size: \", len(test_data_df))\n",
    "\n",
    "# print out top 5 rows of the train set\n",
    "display(train_data_df.head(5))\n",
    "\n",
    "# Remove empty rows from both sets and print out the new lengths\n",
    "train_data_df = train_data_df[~train_data_df[\"ProcessedReview\"].isnull()]\n",
    "test_data_df = test_data_df[~test_data_df[\"ProcessedReview\"].isnull()]\n",
    "print (\"After removing empty Review, train set size: \",len(train_data_df))\n",
    "print (\" After removing empty Review, test set size: \",len(test_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a90aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6920  ...  [('perfect', 4493), ('read', 4912), ('of', 4248), ('the', 6097), ('book', 825), ('have', 2934), ('alway', 363), ('love', 3681), ('fsf', 2609), ('write', 6861), ('hi', 2994), ('prose', 4789), ('is', 3369), ('beauti', 668), ('poetic', 4610), ('thi', 6118), ('best', 720), ('work', 6833), ('by', 959), ('far', 2345), ('come', 1240), ('aliv', 336), ('in', 3185), ('alexand', 330), ('scourbi', 5320), ('listen', 3626), ('to', 6189), ('record', 4960), ('probabl', 4740), ('10', 6), ('time', 6173), ('over', 4344), ('last', 3521), ('few', 2410), ('year', 6888), ('and', 408), ('never', 4119), ('tire', 6180), ('stori', 5815), ('or', 4296), ('way', 6700), ('it', 3379), ('also', 357), ('version', 6603), ('tim', 6172), ('robbin', 5165), ('he', 2940), ('doesn', 1847), ('do', 1841), ('justic', 3443), ('portray', 4647), ('gatsbi', 2674), ('as', 511), ('kind', 3470), ('dour', 1873), ('fellow', 2392), ('view', 6622), ('ultim', 6354), ('romant', 5178), ('that', 6096), ('how', 3068), ('can', 979), ('not', 4180), ('recommend', 4956), ('audio', 563), ('enough', 2110), ('three', 6142), ('star', 5765), ('had', 2877), ('into', 3333), ('no', 4149), ('page', 4387), ('miss', 3937), ('possibl', 4650), ('novel', 4194), ('ever', 2200), ('written', 6863), ('sigh', 5490), ('gasp', 2667), ('ador', 257), ('abandon', 175), ('all', 337), ('respons', 5085), ('gem', 2689), ('straight', 5821), ('through', 6148), ('one', 4272), ('morn', 3993), ('nick', 4133), ('carraway', 1019), ('jay', 3401), ('tom', 6200), ('daisi', 1524), ('becam', 670), ('real', 4918), ('their', 6102), ('messag', 3862), ('reson', 5079), ('so', 5613), ('freedom', 2583)] \n",
      "\n",
      "(2206, 6920) \n",
      "\n",
      "(945, 6920) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use processed reviews for model building\n",
    "y_train = train_data_df[\"sentiment\"]\n",
    "y_test = test_data_df[\"sentiment\"]\n",
    "\n",
    "train_text = train_data_df[\"ProcessedReview\"]\n",
    "test_text = test_data_df[\"ProcessedReview\"]\n",
    "\n",
    "# set the n-gram range\n",
    "vectorizer = CountVectorizer(ngram_range = [1,1])\n",
    "\n",
    "# create training data representation\n",
    "train_data_cv = vectorizer.fit_transform(train_text)\n",
    "\n",
    "# observe the words in the created dictionary across the document\n",
    "print(len(vectorizer.vocabulary_), \" ... \", list(vectorizer.vocabulary_.items())[0:100],\"\\n\")\n",
    "\n",
    "print(train_data_cv.shape,\"\\n\") \n",
    "\n",
    "# create test data representation\n",
    "test_data_cv = vectorizer.transform(test_text)\n",
    "print(test_data_cv.shape,\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b33292eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature space before filtering:  (2206, 6920)\n",
      " Train feature space after filtering:  (2206, 5000)\n",
      " Test feature space before filtering:  (945, 6920)\n",
      "  Test feature space after filtering:  (945, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Use chi-squared statistics to select the best 5000 unigram features\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "selector = SelectKBest(chi2, k=5000)\n",
    "X_train_features_filtered_kbest = selector.fit_transform(train_data_cv, y_train)\n",
    "print (\"Train feature space before filtering: \", train_data_cv.shape)\n",
    "print (\" Train feature space after filtering: \", X_train_features_filtered_kbest.shape)\n",
    "\n",
    "\n",
    "X_test_features_filtered_kbest = selector.transform(test_data_cv)\n",
    "print (\" Test feature space before filtering: \", test_data_cv.shape)\n",
    "print (\"  Test feature space after filtering: \", X_test_features_filtered_kbest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3736a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine filtered datasets to single dataset.\n",
    "from scipy.sparse import vstack\n",
    "X = vstack((X_train_features_filtered_kbest, X_test_features_filtered_kbest))\n",
    "\n",
    "#Combine target data for test and train\n",
    "y = pd.concat([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b1ea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.70, random_state=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc6b13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOverall scores NB\u001b[0m\n",
      " Accuracy score: 0.8446088794926004\n",
      "Precision Score: 0.8167261946288339\n",
      "   Recall score: 0.8446088794926004\n",
      "       F1 score: 0.811114338014781\n",
      "\u001b[1m\n",
      "Individual label performance \u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.66      0.51      0.58       105\n",
      "     neutral       0.57      0.09      0.16        87\n",
      "    positive       0.87      0.98      0.92       754\n",
      "\n",
      "    accuracy                           0.84       946\n",
      "   macro avg       0.70      0.53      0.55       946\n",
      "weighted avg       0.82      0.84      0.81       946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "step1 = MultinomialNB()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('step1', step1)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print('\\033[1m' + 'Overall scores NB' + '\\033[0m')\n",
    "print(\" Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision Score:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"   Recall score:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"       F1 score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print('\\033[1m' + '\\nIndividual label performance ' + '\\033[0m')\n",
    "print (classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7156e166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOverall scores SVM\u001b[0m\n",
      " Accuracy score: 0.8414376321353065\n",
      "Precision Score: 0.8235522411211019\n",
      "   Recall score: 0.8414376321353065\n",
      "       F1 score: 0.830331261998578\n",
      "\u001b[1m\n",
      "Individual label performance SVC Model\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.66      0.54      0.59       105\n",
      "     neutral       0.41      0.29      0.34        87\n",
      "    positive       0.89      0.95      0.92       754\n",
      "\n",
      "    accuracy                           0.84       946\n",
      "   macro avg       0.65      0.59      0.62       946\n",
      "weighted avg       0.82      0.84      0.83       946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step1 = LinearSVC(C=0.1, random_state=17)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('step1', step1)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print('\\033[1m' + 'Overall scores SVM' + '\\033[0m')\n",
    "print(\" Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision Score:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"   Recall score:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"       F1 score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print('\\033[1m' + '\\nIndividual label performance SVC Model' + '\\033[0m')\n",
    "print (classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c50e1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOverall scores\u001b[0m\n",
      " Accuracy score: 0.7928118393234672\n",
      "Precision Score: 0.768579756646761\n",
      "   Recall score: 0.7928118393234672\n",
      "       F1 score: 0.7778274090697043\n",
      "\u001b[1m\n",
      "Individual label performance \u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.39      0.46       105\n",
      "     neutral       0.29      0.21      0.24        87\n",
      "    positive       0.86      0.92      0.88       754\n",
      "\n",
      "    accuracy                           0.79       946\n",
      "   macro avg       0.56      0.50      0.53       946\n",
      "weighted avg       0.77      0.79      0.78       946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step1 = DecisionTreeClassifier(max_depth=43, random_state=17)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('step1', step1)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print('\\033[1m' + 'Overall scores' + '\\033[0m')\n",
    "print(\" Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision Score:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"   Recall score:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"       F1 score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print('\\033[1m' + '\\nIndividual label performance ' + '\\033[0m')\n",
    "print (classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee7a6afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOverall scores\u001b[0m\n",
      " Accuracy score: 0.8276955602536998\n",
      "Precision Score: 0.8025056229867586\n",
      "   Recall score: 0.8276955602536998\n",
      "       F1 score: 0.8044839070209432\n",
      "\u001b[1m\n",
      "Individual label performance \u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.38      0.50       105\n",
      "     neutral       0.38      0.22      0.28        87\n",
      "    positive       0.86      0.96      0.91       754\n",
      "\n",
      "    accuracy                           0.83       946\n",
      "   macro avg       0.66      0.52      0.56       946\n",
      "weighted avg       0.80      0.83      0.80       946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step1 = AdaBoostClassifier(n_estimators=50, learning_rate=.99, random_state=17)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('step1', step1)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print('\\033[1m' + 'Overall scores' + '\\033[0m')\n",
    "print(\" Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision Score:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"   Recall score:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"       F1 score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print('\\033[1m' + '\\nIndividual label performance ' + '\\033[0m')\n",
    "print (classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd97c183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOverall scores\u001b[0m\n",
      " Accuracy score: 0.8181818181818182\n",
      "Precision Score: 0.7947691197691198\n",
      "   Recall score: 0.8181818181818182\n",
      "       F1 score: 0.8016495390552915\n",
      "\u001b[1m\n",
      "Individual label performance \u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.44      0.52       105\n",
      "     neutral       0.40      0.25      0.31        87\n",
      "    positive       0.86      0.94      0.90       754\n",
      "\n",
      "    accuracy                           0.82       946\n",
      "   macro avg       0.63      0.54      0.58       946\n",
      "weighted avg       0.79      0.82      0.80       946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step1 = GradientBoostingClassifier(learning_rate=.5, n_estimators=89, criterion='squared_error',  random_state=17)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('step1', step1)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print('\\033[1m' + 'Overall scores' + '\\033[0m')\n",
    "print(\" Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision Score:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"   Recall score:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"       F1 score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print('\\033[1m' + '\\nIndividual label performance ' + '\\033[0m')\n",
    "print (classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d935dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOverall scores\u001b[0m\n",
      " Accuracy score: 0.8329809725158562\n",
      "Precision Score: 0.8280263336613469\n",
      "   Recall score: 0.8329809725158562\n",
      "       F1 score: 0.7902784350833737\n",
      "\u001b[1m\n",
      "Individual label performance \u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.25      0.38       105\n",
      "     neutral       0.79      0.17      0.28        87\n",
      "    positive       0.83      0.99      0.91       754\n",
      "\n",
      "    accuracy                           0.83       946\n",
      "   macro avg       0.81      0.47      0.52       946\n",
      "weighted avg       0.83      0.83      0.79       946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step1 = ExtraTreesClassifier(n_estimators=100, criterion='gini', min_samples_split=3, random_state=17)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('step1', step1)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print('\\033[1m' + 'Overall scores' + '\\033[0m')\n",
    "print(\" Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision Score:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"   Recall score:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"       F1 score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print('\\033[1m' + '\\nIndividual label performance ' + '\\033[0m')\n",
    "print (classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55ccd29",
   "metadata": {},
   "source": [
    "# Summary  \n",
    "  \n",
    "  \n",
    "<center><img src=\"findings/final.jpg\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225589c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
